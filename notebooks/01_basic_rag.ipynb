{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bf13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden Evaluation Dataset for RAG/Eval Papers\n",
    "# These questions are designed to expose common RAG failure modes\n",
    "\n",
    "EVAL_QUESTIONS = [\n",
    "    # ========== EASY QUESTIONS (Should work with basic RAG) ==========\n",
    "    {\n",
    "        \"id\": \"easy_01\",\n",
    "        \"question\": \"What does RAG stand for?\",\n",
    "        \"expected_answer\": \"Retrieval-Augmented Generation\",\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"failure_mode\": None,\n",
    "        \"source_papers\": [\"original_rag_2020.pdf\"],\n",
    "        \"notes\": \"Basic fact, should work\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"easy_02\",\n",
    "        \"question\": \"What is the main purpose of RAG systems?\",\n",
    "        \"expected_answer\": \"To augment LLM responses with retrieved external knowledge to improve accuracy and reduce hallucinations\",\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"failure_mode\": None,\n",
    "        \"source_papers\": [\"original_rag_2020.pdf\"],\n",
    "        \"notes\": \"Core concept\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"easy_03\",\n",
    "        \"question\": \"What are the main components of a RAG system?\",\n",
    "        \"expected_answer\": \"A retriever (to find relevant documents) and a generator (LLM to produce answers using retrieved context)\",\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"failure_mode\": None,\n",
    "        \"source_papers\": [\"original_rag_2020.pdf\"],\n",
    "        \"notes\": \"Fundamental architecture\"\n",
    "    },\n",
    "    \n",
    "    # ========== MEDIUM - SEMANTIC CONFUSION ==========\n",
    "    {\n",
    "        \"id\": \"semantic_01\",\n",
    "        \"question\": \"How does Self-RAG differ from standard RAG?\",\n",
    "        \"expected_answer\": \"Self-RAG adds reflection tokens that let the model decide when to retrieve, what to retrieve, and whether retrieved content is useful, making it more autonomous than standard RAG\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"semantic_confusion\",\n",
    "        \"source_papers\": [\"self_rag_2023.pdf\", \"original_rag_2020.pdf\"],\n",
    "        \"notes\": \"Requires distinguishing two similar concepts\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"semantic_02\",\n",
    "        \"question\": \"What is the difference between RAGAS and traditional RAG evaluation?\",\n",
    "        \"expected_answer\": \"RAGAS is an automated evaluation framework that measures faithfulness, answer relevance, and context precision/recall, whereas traditional evaluation often relies on manual human assessment\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"semantic_confusion\",\n",
    "        \"source_papers\": [\"ragas_paper.pdf\"],\n",
    "        \"notes\": \"Similar terminology, different meanings\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"semantic_03\",\n",
    "        \"question\": \"What is HyDE and how does it work?\",\n",
    "        \"expected_answer\": \"Hypothetical Document Embeddings (HyDE) generates a hypothetical answer to the query first, then uses that answer for retrieval instead of the query itself, improving semantic matching\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"semantic_confusion\",\n",
    "        \"source_papers\": [\"hyde.pdf\"],\n",
    "        \"notes\": \"Technical concept that might get confused with other retrieval methods\"\n",
    "    },\n",
    "    \n",
    "    # ========== MEDIUM - CONTRADICTORY/TEMPORAL ==========\n",
    "    {\n",
    "        \"id\": \"temporal_01\",\n",
    "        \"question\": \"What are the most recent advances in RAG architectures?\",\n",
    "        \"expected_answer\": \"Recent advances include Self-RAG (2023), Corrective RAG/CRAG (2024), Graph RAG (2024), and agentic RAG systems with iterative retrieval\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"temporal_confusion\",\n",
    "        \"source_papers\": [\"graph_rag_2024.pdf\", \"crag_2024.pdf\", \"self_rag_2023.pdf\"],\n",
    "        \"notes\": \"Needs to retrieve from recent papers, not old ones\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"temporal_02\",\n",
    "        \"question\": \"How has RAG evaluation evolved over time?\",\n",
    "        \"expected_answer\": \"Early RAG evaluation relied on QA accuracy metrics, while modern approaches like RAGAS and ARES focus on component-level metrics (faithfulness, relevance) and automated LLM-as-judge techniques\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"temporal_confusion\",\n",
    "        \"source_papers\": [\"ragas_paper.pdf\", \"ares_paper.pdf\"],\n",
    "        \"notes\": \"Requires understanding evolution\"\n",
    "    },\n",
    "    \n",
    "    # ========== HARD - MULTI-HOP REASONING ==========\n",
    "    {\n",
    "        \"id\": \"multihop_01\",\n",
    "        \"question\": \"If I want to build a RAG system that can self-correct its retrieval, which architecture should I use and how would I evaluate it?\",\n",
    "        \"expected_answer\": \"Use Corrective RAG (CRAG) which evaluates retrieval quality and decides whether to refine queries or use web search. Evaluate using RAGAS metrics for faithfulness and answer correctness, plus CRAG-specific metrics for retrieval correction rate\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"missing_context\",\n",
    "        \"source_papers\": [\"crag_2024.pdf\", \"ragas_paper.pdf\"],\n",
    "        \"notes\": \"Needs info from 2 different papers\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"multihop_02\",\n",
    "        \"question\": \"What are the trade-offs between Dense Passage Retrieval and HyDE for semantic search?\",\n",
    "        \"expected_answer\": \"DPR requires training on query-document pairs and works well with large labeled datasets, while HyDE is zero-shot and generates hypothetical documents but depends on LLM quality. DPR is more precise with training data; HyDE is more flexible without it\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"missing_context\",\n",
    "        \"source_papers\": [\"dense_passage_retrieval.pdf\", \"hyde.pdf\"],\n",
    "        \"notes\": \"Compare/contrast two papers\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"multihop_03\",\n",
    "        \"question\": \"How would you combine Graph RAG with LLM-as-judge evaluation?\",\n",
    "        \"expected_answer\": \"Graph RAG builds knowledge graphs for better relationship understanding. Evaluate it using LLM-as-judge to assess: (1) whether retrieved graph segments are relevant, (2) if relationships are accurately represented, (3) answer faithfulness to graph structure\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"missing_context\",\n",
    "        \"source_papers\": [\"graph_rag_2024.pdf\", \"llm_as_judge.pdf\"],\n",
    "        \"notes\": \"Synthesize concepts from multiple papers\"\n",
    "    },\n",
    "    \n",
    "    # ========== HARD - SPARSE/TECHNICAL DETAILS ==========\n",
    "    {\n",
    "        \"id\": \"sparse_01\",\n",
    "        \"question\": \"What is the mathematical formulation for computing context precision in RAGAS?\",\n",
    "        \"expected_answer\": \"Context Precision = (Sum of precision@k for all relevant chunks) / (Total number of relevant chunks in ground truth), where precision@k measures relevant chunks in top-k retrievals\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"sparse_info\",\n",
    "        \"source_papers\": [\"ragas_paper.pdf\"],\n",
    "        \"notes\": \"Very specific technical detail, likely buried in paper\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"sparse_02\",\n",
    "        \"question\": \"What hyperparameters does the original RAG paper use for DPR training?\",\n",
    "        \"expected_answer\": \"The paper uses learning rate 1e-5, batch size 128, max sequence length 256 for passages, and trains for 40 epochs on Natural Questions dataset\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"sparse_info\",\n",
    "        \"source_papers\": [\"original_rag_2020.pdf\"],\n",
    "        \"notes\": \"Specific numbers often in appendix or tables\"\n",
    "    },\n",
    "    \n",
    "    # ========== HARD - EDGE CASES ==========\n",
    "    {\n",
    "        \"id\": \"edge_01\",\n",
    "        \"question\": \"What are the failure modes of RAG systems mentioned across different papers?\",\n",
    "        \"expected_answer\": \"Common failures include: retrieval of irrelevant chunks, hallucination when context is insufficient, 'lost in the middle' problem with long contexts, contradictory information from multiple sources, and poor performance on multi-hop reasoning\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"synthesis\",\n",
    "        \"source_papers\": [\"multiple papers\"],\n",
    "        \"notes\": \"Requires synthesizing info from many papers\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"edge_02\",\n",
    "        \"question\": \"Which RAG papers discuss handling contradictory information in retrieved documents?\",\n",
    "        \"expected_answer\": \"Corrective RAG (CRAG) and Self-RAG both address contradictory information - CRAG filters low-quality retrievals and Self-RAG uses reflection to assess usefulness of retrieved content\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"edge_case\",\n",
    "        \"source_papers\": [\"crag_2024.pdf\", \"self_rag_2023.pdf\"],\n",
    "        \"notes\": \"Uncommon topic, might be mentioned briefly\"\n",
    "    },\n",
    "    \n",
    "    # ========== COMPARISON QUESTIONS ==========\n",
    "    {\n",
    "        \"id\": \"compare_01\",\n",
    "        \"question\": \"Compare the evaluation approaches in RAGAS vs ARES papers\",\n",
    "        \"expected_answer\": \"RAGAS uses aspect-based evaluation (faithfulness, relevance, context metrics) with synthetic data generation. ARES focuses on automated evaluation with confidence estimation and handles few-shot learning. Both use LLMs for automated assessment but ARES emphasizes statistical confidence\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"comparison\",\n",
    "        \"source_papers\": [\"ragas_paper.pdf\", \"ares_paper.pdf\"],\n",
    "        \"notes\": \"Direct comparison of two frameworks\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"compare_02\",\n",
    "        \"question\": \"What are the pros and cons of Self-RAG vs Corrective RAG?\",\n",
    "        \"expected_answer\": \"Self-RAG: Pros - adaptive retrieval, learns when to retrieve. Cons - requires training reflection tokens. CRAG: Pros - can use external sources for correction, post-hoc evaluation. Cons - adds latency with correction step. Self-RAG is more integrated; CRAG is more modular\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"comparison\",\n",
    "        \"source_papers\": [\"self_rag_2023.pdf\", \"crag_2024.pdf\"],\n",
    "        \"notes\": \"Architectural trade-offs\"\n",
    "    },\n",
    "    \n",
    "    # ========== QUESTIONS THAT SHOULD FAIL BASELINE ==========\n",
    "    {\n",
    "        \"id\": \"fail_01\",\n",
    "        \"question\": \"What does the RAGAS paper say about context relevance?\",\n",
    "        \"expected_answer\": \"RAGAS defines context relevance as whether retrieved chunks contain information needed to answer the query. It measures this by checking if each sentence in the context can be attributed to answering the question\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"should_fail_baseline\",\n",
    "        \"source_papers\": [\"ragas_paper.pdf\"],\n",
    "        \"notes\": \"Specific term that might retrieve wrong papers mentioning 'context' or 'relevance'\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"fail_02\",\n",
    "        \"question\": \"How do you prevent RAG systems from hallucinating?\",\n",
    "        \"expected_answer\": \"Multiple approaches: (1) Use faithfulness metrics in evaluation (RAGAS), (2) Add reflection/self-critique (Self-RAG), (3) Implement retrieval quality checks (CRAG), (4) Use constrained generation that stays within retrieved context\",\n",
    "        \"difficulty\": \"hard\",\n",
    "        \"failure_mode\": \"should_fail_baseline\",\n",
    "        \"source_papers\": [\"multiple papers\"],\n",
    "        \"notes\": \"Broad question requiring synthesis - baseline will likely give generic answer\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"fail_03\",\n",
    "        \"question\": \"What specific metrics does ARES use for RAG evaluation?\",\n",
    "        \"expected_answer\": \"ARES uses three main metrics: Context Relevance (is retrieved context relevant?), Answer Faithfulness (does answer use only retrieved info?), and Answer Relevance (does answer address the question?). It also provides confidence scores for each metric\",\n",
    "        \"difficulty\": \"medium\",\n",
    "        \"failure_mode\": \"should_fail_baseline\",\n",
    "        \"source_papers\": [\"ares_paper.pdf\"],\n",
    "        \"notes\": \"Specific to ARES - might confuse with RAGAS metrics\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b56153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Distribution:\n",
      "Total Questions: 20\n",
      "\n",
      "By Difficulty:\n",
      "  easy: 3\n",
      "  medium: 7\n",
      "  hard: 10\n",
      "\n",
      "By Failure Mode:\n",
      "  semantic_confusion: 3\n",
      "  temporal_confusion: 2\n",
      "  missing_context: 3\n",
      "  sparse_info: 2\n",
      "  edge_case: 1\n",
      "  comparison: 2\n",
      "  should_fail_baseline: 3\n"
     ]
    }
   ],
   "source": [
    "# ========== METADATA FOR ANALYSIS ==========\n",
    "\n",
    "# Count by difficulty\n",
    "difficulty_distribution = {\n",
    "    \"easy\": len([q for q in EVAL_QUESTIONS if q[\"difficulty\"] == \"easy\"]),\n",
    "    \"medium\": len([q for q in EVAL_QUESTIONS if q[\"difficulty\"] == \"medium\"]),\n",
    "    \"hard\": len([q for q in EVAL_QUESTIONS if q[\"difficulty\"] == \"hard\"])\n",
    "}\n",
    "\n",
    "# Count by failure mode\n",
    "failure_mode_distribution = {\n",
    "    \"semantic_confusion\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"semantic_confusion\"]),\n",
    "    \"temporal_confusion\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"temporal_confusion\"]),\n",
    "    \"missing_context\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"missing_context\"]),\n",
    "    \"sparse_info\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"sparse_info\"]),\n",
    "    \"edge_case\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"edge_case\"]),\n",
    "    \"comparison\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"comparison\"]),\n",
    "    \"should_fail_baseline\": len([q for q in EVAL_QUESTIONS if q.get(\"failure_mode\") == \"should_fail_baseline\"])\n",
    "}\n",
    "\n",
    "print(\"Question Distribution:\")\n",
    "print(f\"Total Questions: {len(EVAL_QUESTIONS)}\")\n",
    "print(f\"\\nBy Difficulty:\")\n",
    "for diff, count in difficulty_distribution.items():\n",
    "    print(f\"  {diff}: {count}\")\n",
    "print(f\"\\nBy Failure Mode:\")\n",
    "for mode, count in failure_mode_distribution.items():\n",
    "    print(f\"  {mode}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e25bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE RAG (Deliberately Flawed)\n",
    "# ============================================================================\n",
    "\n",
    "class BaselineRAG:\n",
    "    \"\"\"\n",
    "    Basic RAG with known issues:\n",
    "    - Small chunk size (causes context loss)\n",
    "    - Low top_k (misses relevant chunks)\n",
    "    - No query rewriting\n",
    "    - No reranking\n",
    "    - No metadata filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path):\n",
    "        # Use small, problematic settings\n",
    "        Settings.llm = Ollama(\n",
    "            model=\"llama3.2\", \n",
    "            base_url=\"http://127.0.0.1:11434\",\n",
    "            request_timeout=300.0\n",
    "        )\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        Settings.chunk_size = 512  # DELIBERATELY SMALL - causes issues\n",
    "        Settings.chunk_overlap = 50  # DELIBERATELY SMALL\n",
    "        \n",
    "        # Load documents\n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir=str(data_dir),\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=False\n",
    "        )\n",
    "        self.documents = loader.load_data()\n",
    "        \n",
    "        # Build index\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents)\n",
    "        \n",
    "    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Basic query - no enhancements\"\"\"\n",
    "        query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=top_k,  # DELIBERATELY LOW\n",
    "            response_mode=\"compact\"\n",
    "        )\n",
    "        \n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": str(response),\n",
    "            \"source_nodes\": [\n",
    "                {\n",
    "                    \"text\": node.text[:300],\n",
    "                    \"score\": node.score,\n",
    "                    \"metadata\": node.metadata\n",
    "                }\n",
    "                for node in response.source_nodes\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1fe9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED RAG V1: Query Rewriting\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedRAG_V1(BaselineRAG):\n",
    "    \"\"\"\n",
    "    Enhancement 1: Query Rewriting\n",
    "    - Rephrases user query to match document language\n",
    "    - Uses LLM to generate better search query\n",
    "    \"\"\"\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        # Rewrite query before retrieval\n",
    "        rewrite_prompt = f\"\"\"\n",
    "        Rewrite this question as a search query that would match formal documentation.\n",
    "        Use technical/formal language.\n",
    "        \n",
    "        Original question: {question}\n",
    "        \n",
    "        Rewritten query (be concise, 1 sentence):\n",
    "        \"\"\"\n",
    "        \n",
    "        rewritten = Settings.llm.complete(rewrite_prompt)\n",
    "        rewritten_query = str(rewritten).strip()\n",
    "        \n",
    "        # Use rewritten query for retrieval\n",
    "        query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=top_k,\n",
    "            response_mode=\"compact\"\n",
    "        )\n",
    "        \n",
    "        response = query_engine.query(rewritten_query)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": str(response),\n",
    "            \"rewritten_query\": rewritten_query,\n",
    "            \"source_nodes\": [\n",
    "                {\n",
    "                    \"text\": node.text[:300],\n",
    "                    \"score\": node.score,\n",
    "                    \"metadata\": node.metadata\n",
    "                }\n",
    "                for node in response.source_nodes\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c26536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED RAG V2: Increased Retrieval + Better Chunking\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedRAG_V2:\n",
    "    \"\"\"\n",
    "    Enhancement 2: Better Retrieval Settings\n",
    "    - Larger chunks (more context)\n",
    "    - More overlap (continuity)\n",
    "    - Higher top_k (broader search)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path):\n",
    "        Settings.llm = Ollama(\n",
    "            model=\"llama3.2\", \n",
    "            base_url=\"http://127.0.0.1:11434\",\n",
    "            request_timeout=300.0\n",
    "        )\n",
    "        Settings.embed_model = HuggingFaceEmbedding(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        Settings.chunk_size = 1024  # LARGER\n",
    "        Settings.chunk_overlap = 200  # MORE OVERLAP\n",
    "        \n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir=str(data_dir),\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=False\n",
    "        )\n",
    "        self.documents = loader.load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents)\n",
    "        \n",
    "    def query(self, question: str, top_k: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Query with higher top_k\"\"\"\n",
    "        # Rewrite query\n",
    "        rewrite_prompt = f\"\"\"\n",
    "        Rewrite this question as a search query for formal documentation.\n",
    "        Original: {question}\n",
    "        Rewritten (concise):\n",
    "        \"\"\"\n",
    "        rewritten = Settings.llm.complete(rewrite_prompt)\n",
    "        rewritten_query = str(rewritten).strip()\n",
    "        \n",
    "        # Retrieve MORE chunks\n",
    "        query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=top_k,  # INCREASED from 3 to 10\n",
    "            response_mode=\"compact\"\n",
    "        )\n",
    "        \n",
    "        response = query_engine.query(rewritten_query)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": str(response),\n",
    "            \"rewritten_query\": rewritten_query,\n",
    "            \"source_nodes\": [\n",
    "                {\n",
    "                    \"text\": node.text[:300],\n",
    "                    \"score\": node.score,\n",
    "                    \"metadata\": node.metadata\n",
    "                }\n",
    "                for node in response.source_nodes\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e93d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED RAG V3: Better Prompting\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedRAG_V3(ImprovedRAG_V2):\n",
    "    \"\"\"\n",
    "    Enhancement 3: Structured Prompting\n",
    "    - Clear instructions\n",
    "    - Format requirements\n",
    "    - Citation requirements\n",
    "    \"\"\"\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 10) -> Dict[str, Any]:\n",
    "        from llama_index.core import PromptTemplate\n",
    "        \n",
    "        # Define structured prompt\n",
    "        qa_prompt_str = \"\"\"\n",
    "Context information is below:\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Instructions:\n",
    "- Answer the question using ONLY the context above\n",
    "- If the context doesn't contain the answer, say \"I don't have enough information\"\n",
    "- Be specific and cite which document/section you're using\n",
    "- Format your answer clearly\n",
    "\n",
    "Question: {query_str}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        qa_prompt = PromptTemplate(qa_prompt_str)\n",
    "        \n",
    "        # Rewrite query\n",
    "        rewrite_prompt = f\"\"\"\n",
    "        Rewrite this question as a search query for formal documentation.\n",
    "        Original: {question}\n",
    "        Rewritten (concise):\n",
    "        \"\"\"\n",
    "        rewritten = Settings.llm.complete(rewrite_prompt)\n",
    "        rewritten_query = str(rewritten).strip()\n",
    "        \n",
    "        # Query with custom prompt\n",
    "        query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=top_k,\n",
    "            response_mode=\"compact\",\n",
    "            text_qa_template=qa_prompt\n",
    "        )\n",
    "        \n",
    "        response = query_engine.query(rewritten_query)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": str(response),\n",
    "            \"rewritten_query\": rewritten_query,\n",
    "            \"source_nodes\": [\n",
    "                {\n",
    "                    \"text\": node.text[:300],\n",
    "                    \"score\": node.score,\n",
    "                    \"metadata\": node.metadata\n",
    "                }\n",
    "                for node in response.source_nodes\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a36962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FRAMEWORK\n",
    "# ============================================================================\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates RAG systems on your golden dataset\n",
    "    Tracks improvements across versions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        \n",
    "    def evaluate_system(\n",
    "        self, \n",
    "        system, \n",
    "        system_name: str,\n",
    "        questions: List[Dict]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Run all questions through a RAG system\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating: {system_name}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        system_results = {\n",
    "            \"system_name\": system_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"questions\": []\n",
    "        }\n",
    "        \n",
    "        for q in questions:\n",
    "            print(f\"Q: {q['question']}\")\n",
    "            \n",
    "            try:\n",
    "                result = system.query(q[\"question\"])\n",
    "                \n",
    "                question_result = {\n",
    "                    \"question_id\": q[\"id\"],\n",
    "                    \"question\": q[\"question\"],\n",
    "                    \"expected\": q[\"expected_answer\"],\n",
    "                    \"actual\": result[\"answer\"],\n",
    "                    \"difficulty\": q[\"difficulty\"],\n",
    "                    \"failure_mode\": q.get(\"failure_mode\"),\n",
    "                    \"sources_retrieved\": len(result[\"source_nodes\"]),\n",
    "                    \"top_source_score\": result[\"source_nodes\"][0][\"score\"] if result[\"source_nodes\"] else 0,\n",
    "                    \"rewritten_query\": result.get(\"rewritten_query\", None)\n",
    "                }\n",
    "                \n",
    "                # Manual assessment (you'll review these)\n",
    "                print(f\"A: {result['answer'][:200]}...\")\n",
    "                print(f\"Sources: {len(result['source_nodes'])} chunks retrieved\")\n",
    "                print()\n",
    "                \n",
    "                system_results[\"questions\"].append(question_result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {e}\\n\")\n",
    "                question_result = {\n",
    "                    \"question_id\": q[\"id\"],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                system_results[\"questions\"].append(question_result)\n",
    "        \n",
    "        # Save results\n",
    "        results_file = RESULTS_DIR / f\"{system_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(system_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "        \n",
    "        return system_results\n",
    "    \n",
    "    def compare_systems(self, results_files: List[Path]):\n",
    "        \"\"\"Compare multiple system results side-by-side\"\"\"\n",
    "        # TODO: Implement comparison logic\n",
    "        # This will generate your \"before/after\" tables for blog posts\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def run_experiments():\n",
    "    \"\"\"\n",
    "    Run all experiments and collect results\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluator = RAGEvaluator()\n",
    "    \n",
    "    # Experiment 1: Baseline (should show problems)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 1: Baseline RAG\")\n",
    "    print(\"=\"*60)\n",
    "    baseline = BaselineRAG(DATA_RAW_DIR)\n",
    "    baseline_results = evaluator.evaluate_system(\n",
    "        baseline, \n",
    "        \"baseline\",\n",
    "        EVAL_QUESTIONS[:5]  # Start with 5 questions\n",
    "    )\n",
    "    \n",
    "    # Experiment 2: Query Rewriting\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 2: With Query Rewriting\")\n",
    "    print(\"=\"*60)\n",
    "    improved_v1 = ImprovedRAG_V1(DATA_RAW_DIR)\n",
    "    v1_results = evaluator.evaluate_system(\n",
    "        improved_v1,\n",
    "        \"query_rewriting\",\n",
    "        EVAL_QUESTIONS[:5]\n",
    "    )\n",
    "    \n",
    "    # Experiment 3: Better Retrieval Settings\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 3: Better Chunking + Higher top_k\")\n",
    "    print(\"=\"*60)\n",
    "    improved_v2 = ImprovedRAG_V2(DATA_RAW_DIR)\n",
    "    v2_results = evaluator.evaluate_system(\n",
    "        improved_v2,\n",
    "        \"improved_retrieval\",\n",
    "        EVAL_QUESTIONS[:5]\n",
    "    )\n",
    "    \n",
    "    # Experiment 4: Better Prompting\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT 4: Structured Prompting\")\n",
    "    print(\"=\"*60)\n",
    "    improved_v3 = ImprovedRAG_V3(DATA_RAW_DIR)\n",
    "    v3_results = evaluator.evaluate_system(\n",
    "        improved_v3,\n",
    "        \"structured_prompting\",\n",
    "        EVAL_QUESTIONS[:5]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"All experiments complete!\")\n",
    "    print(f\"Results saved in: {RESULTS_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run this to generate all your experimental data\n",
    "    run_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
