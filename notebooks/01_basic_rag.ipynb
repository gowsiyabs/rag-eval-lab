{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500f5ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\gowsi\\OneDrive\\Documents\\my-git-code\\rag-eval-lab\n",
      "Raw data dir: c:\\Users\\gowsi\\OneDrive\\Documents\\my-git-code\\rag-eval-lab\\data\\raw\n",
      "Files: [WindowsPath('c:/Users/gowsi/OneDrive/Documents/my-git-code/rag-eval-lab/data/raw/Evals_Paper.pdf')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Set base paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Raw data dir:\", DATA_RAW_DIR)\n",
    "print(\"Files:\", list(DATA_RAW_DIR.iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b50ca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 22:49:21,957 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Configure global LlamaIndex settings (you can tune later)\n",
    "# Settings.llm = OpenAI(model=\"gpt-4o-mini\")  # cheap + decent for RAG\n",
    "# Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Local LLM via Ollama\n",
    "Settings.llm = Ollama(model=\"llama3.2\", \n",
    "                      base_url=\"http://127.0.0.1:11434\",  # be explicit\n",
    "                      request_timeout=300.0 )  # or another small model you pulled\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "Settings.chunk_size = 1024   # can experiment with this later\n",
    "Settings.chunk_overlap = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198b969e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " {'page_label': '1',\n",
       "  'file_name': 'Evals_Paper.pdf',\n",
       "  'file_path': 'c:\\\\Users\\\\gowsi\\\\OneDrive\\\\Documents\\\\my-git-code\\\\rag-eval-lab\\\\data\\\\raw\\\\Evals_Paper.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 2613163,\n",
       "  'creation_date': '2025-11-25',\n",
       "  'last_modified_date': '2025-11-25'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the paper\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=str(DATA_RAW_DIR),\n",
    "    required_exts=[\".pdf\"],      # only PDFs\n",
    "    recursive=False\n",
    ")\n",
    "\n",
    "documents = loader.load_data()\n",
    "len(documents), documents[0].metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324f89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 22:49:26,545 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/show \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Build index over the paper\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a simple query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,   # number of chunks to retrieve\n",
    "    response_mode=\"compact\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc45617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 22:50:16,038 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper discusses various evaluation methods for LLMs, including:\n",
      "\n",
      "1. Automating Evaluations of Prompts\n",
      "2. Using automated evaluation pipelines in code or with other LLMs\n",
      "3. Public PE tools like promptfoo and ChainForge that support both code-based and LLM-based evaluators\n",
      "4. Assertion selectivity and its impact on LLM output quality confidence\n",
      "5. Sampling grades from LLM outputs, including strategies such as random, highest, lowest, and alternating sampling policies\n",
      "6. Choosing aligned assertions for each criterion, using the highest alignment score among candidate assertions\n"
     ]
    }
   ],
   "source": [
    "question = \"What types of evaluation methods does this paper discuss for LLMs?\"\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "response = query_engine.query(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01fc5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Source 1 | score=0.5815 ---\n",
      "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences InputsPrompt Under TestLLM OutputsMetrics Evaluator LLMMetric Prompt Metric (Code)… Test Results … iterate (a) Typical Evaluation Pipeline InputsPrompt Under TestLLM Outputs Test Results Candidate Criteria Candidate Assertions edit criteria grade LLM outputs Selected Assertions Alignment Report Car\n",
      "\n",
      "--- Source 2 | score=0.5451 ---\n",
      "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences Figure 3: The Table View, showing inputs, LLM outputs, and evaluation results per criteria for the NER task (Sec. 6). generated no good assertions, and participants deleted the criteria without complaints. P8 said, “I like that it tries, because maybe there will be a good implementation!” This sugg\n",
      "\n",
      "--- Source 3 | score=0.5434 ---\n",
      "One could imagine interfaces similar to creating a “pull request” for a new assertion and soliciting review from a team member, and a workflow similar to continuous integration/continuous deployment (CI/CD) that seamlessly pushes new assertions to production. 8.3 Future Work and Limitations Assertions serve as a straightforward mechanism for evaluating LLM outputs, yet the potential of evaluation \n",
      "\n",
      "--- Source 4 | score=0.5339 ---\n",
      "Shreya Shankar, J.D. Zamfirescu-Pereira, Björn Hartmann, Aditya G. Parameswaran, and Ian Arawjo LLMs in production contexts. Because our participants were indus- try practitioners and thus possibly dealing with NDA-protected data, we offered a task adapted from a real LLM pipeline prompt. Our study design did not impose restrictions on how participants used EvalGen, and users could choose whether \n",
      "\n",
      "--- Source 5 | score=0.5244 ---\n",
      "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences A ALGORITHMS FOR SELECTING ASSERTIONS & ELICITING GRADES A.1 Assertion Selectivity and Impact on LLM Output Quality Confidence One way to establish confidence in whether an LLM output is problematic is to assess the selectivity, or pass rate, of assertions that fail it. Intuitively, assertions that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# response.source_nodes contains retrieved chunks + metadata\n",
    "for i, node in enumerate(response.source_nodes, start=1):\n",
    "    print(f\"--- Source {i} | score={node.score:.4f} ---\")\n",
    "    # Print only first 400 chars for readability\n",
    "    print(node.text[:400].replace(\"\\n\", \" \"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f280b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
