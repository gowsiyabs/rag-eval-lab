{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32264c25",
   "metadata": {},
   "source": [
    "### Evaluating Explanations with an LLM Evaluator (Ollama)\n",
    "\n",
    "This notebook experiments with **LLM-based product evals** using a small labeled dataset of explanations for an article.\n",
    "\n",
    "Runtime: Ollama\n",
    "To run this notebook:\n",
    "\n",
    "- Install and open **Ollama**\n",
    "- Pull and load the model:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "ollama run llama3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54eac2",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "This flow is inspired by Eugene Yan’s article:\n",
    "\n",
    "> **“How I Build Product Evaluations for LLM Applications”**  \n",
    "> https://eugeneyan.com/writing/product-evals/\n",
    "\n",
    "I follow his ideas of:\n",
    "\n",
    "- Labeling a **small dataset** (with enough fails)  \n",
    "- Using **binary labels** (`PASS` / `FAIL`)  \n",
    "- Treating the **evaluator as a model to align** and measuring metrics  \n",
    "\n",
    "---\n",
    "\n",
    "#### What this notebook does\n",
    "\n",
    "- Loads `data/evals/article_explanations.csv` with:\n",
    "  - `input` – paragraph/snippet from the article  \n",
    "  - `output` – a candidate explanation  \n",
    "  - `label` – human ground truth (`PASS` / `FAIL`)\n",
    "- Uses an **LLM via Ollama** as an evaluator:\n",
    "  - Given `input` + `output`, it predicts `pred_label` (`PASS` / `FAIL`)\n",
    "- Computes metrics (with **FAIL as the “positive” class**):\n",
    "  - Precision / Recall / F1 for **FAIL**  \n",
    "  - Cohen’s Kappa between `label` and `pred_label`\n",
    "\n",
    "This gives a small, reusable **eval harness** for the task:\n",
    "\n",
    "> *“Is this a good explanation of this paragraph?”*\n",
    "\n",
    "---\n",
    "\n",
    "#### How to read the metrics\n",
    "\n",
    "- **Precision (FAIL)** – of the explanations marked FAIL, how many are truly bad?  \n",
    "- **Recall (FAIL)** – of all truly bad explanations, how many did we catch?  \n",
    "- **F1 (FAIL)** – balance of precision and recall for FAIL.  \n",
    "- **Kappa** – agreement between human and LLM labels  \n",
    "  - Higher is better; ~0.4–0.6 is already decent.\n",
    "\n",
    "---\n",
    "\n",
    "#### Future work\n",
    "\n",
    "- Add more examples (especially realistic FAILs).  \n",
    "- Try different evaluator prompts and compare metrics.  \n",
    "- Split into multiple evaluators (e.g., **faithfulness** vs **main idea** vs **conciseness**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9985f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval file exists: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The author recommends using simple binary labe...</td>\n",
       "      <td>The author argues that we should always use de...</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The author recommends using simple binary labe...</td>\n",
       "      <td>The key message is to prefer simple, discrete ...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>He points out that fine-grained numeric scales...</td>\n",
       "      <td>The article says numeric rating scales are ide...</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>He notes that stakeholders sometimes ask for g...</td>\n",
       "      <td>The article observes that stakeholders often a...</td>\n",
       "      <td>PASS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              input  \\\n",
       "0   1  The author recommends using simple binary labe...   \n",
       "1   2  The author recommends using simple binary labe...   \n",
       "2   3  He points out that fine-grained numeric scales...   \n",
       "3   4  He notes that stakeholders sometimes ask for g...   \n",
       "\n",
       "                                              output label  \n",
       "0  The author argues that we should always use de...  FAIL  \n",
       "1  The key message is to prefer simple, discrete ...  PASS  \n",
       "2  The article says numeric rating scales are ide...  FAIL  \n",
       "3  The article observes that stakeholders often a...  PASS  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_EVALS = PROJECT_ROOT / \"data\" / \"evals\" / \"article_explanations.csv\"\n",
    "\n",
    "# print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Eval file exists:\", DATA_EVALS.exists())\n",
    "\n",
    "df = pd.read_csv(DATA_EVALS)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142af78",
   "metadata": {},
   "source": [
    "Step 2: Align an evaluator LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ea6bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "# Setup Ollama LLM Evaluator\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "eval_llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://127.0.0.1:11434\",\n",
    "    request_timeout=300.0,\n",
    ")\n",
    "\n",
    "# quick sanity check\n",
    "resp = eval_llm.complete(\"Say hello in one short sentence.\")\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e6503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the evaluator prompt - output should be PASS or FAIL\n",
    "def build_eval_prompt(row):\n",
    "    return f\"\"\"\n",
    "You are evaluating an explanation of a paragraph from an article about building product evaluations.\n",
    "\n",
    "ARTICLE PARAGRAPH:\n",
    "\\\"\\\"\\\"{row['input']}\\\"\\\"\\\"\n",
    "\n",
    "MODEL EXPLANATION:\n",
    "\\\"\\\"\\\"{row['output']}\\\"\\\"\\\"\n",
    "\n",
    "EVALUATION CRITERION:\n",
    "- The explanation should be accurate and faithful to the paragraph.\n",
    "- It should clearly capture the *main idea* the author is making.\n",
    "- It should be concise (around 2–3 sentences).\n",
    "- If it misses key points, invents claims, or is vague/general, it should be FAIL.\n",
    "\n",
    "Respond with exactly one word: PASS or FAIL.\n",
    "\"\"\"\n",
    "\n",
    "def llm_evaluate(row):\n",
    "    prompt = build_eval_prompt(row)\n",
    "    resp = eval_llm.complete(prompt)\n",
    "    return resp.text.strip().upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7904b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev size: 12 Test size: 5\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluator on a subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_dev, df_test = train_test_split(df, test_size=0.25, random_state=42, stratify=df['label'])\n",
    "\n",
    "print(\"Dev size:\", len(df_dev), \"Test size:\", len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df_dev.copy()\n",
    "df_dev['pred_label'] = df_dev.apply(llm_evaluate, axis=1)\n",
    "\n",
    "# Normalize true labels\n",
    "df_dev['label'] = df_dev['label'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Normalize predicted labels: strip, uppercase, remove trailing punctuation\n",
    "df_dev['pred_label'] = (\n",
    "    df_dev['pred_label']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str.replace('.', '', regex=False)   # remove dots\n",
    ")\n",
    "\n",
    "print(\"True labels:\\n\", df_dev['label'].value_counts())\n",
    "print(\"\\nPred labels:\\n\", df_dev['pred_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34307bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (FAIL): 1.000\n",
      "Recall (FAIL):    0.857\n",
      "F1 (FAIL):        0.923\n",
      "Cohen's Kappa:    0.833\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, and Cohen’s Kappa\n",
    "from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score\n",
    "\n",
    "y_true = df_dev['label']\n",
    "y_pred = df_dev['pred_label']\n",
    "\n",
    "# Treat FAIL as the \"positive\" class – convert to booleans\n",
    "y_true_fail = (y_true == 'FAIL')\n",
    "y_pred_fail = (y_pred == 'FAIL')\n",
    "\n",
    "# treat FAIL as the \"positive\" class (since we care about catching defects)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=['FAIL', 'PASS'], pos_label='FAIL', average='binary'\n",
    ")\n",
    "\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision (FAIL): {precision:.3f}\")\n",
    "print(f\"Recall (FAIL):    {recall:.3f}\")\n",
    "print(f\"F1 (FAIL):        {f1:.3f}\")\n",
    "print(f\"Cohen's Kappa:    {kappa:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af45180",
   "metadata": {},
   "source": [
    "Step 3: Wrap it in a tiny “eval harness”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840092d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 'baseline_prompt',\n",
       " 'n_samples': 5,\n",
       " 'precision_FAIL': 0.75,\n",
       " 'recall_FAIL': 1.0,\n",
       " 'f1_FAIL': 0.8571428571428571,\n",
       " 'kappa': 0.5454545454545454}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def run_eval(df_samples, name=\"run\"):\n",
    "     # handle empty test set\n",
    "    if df_samples is None or len(df_samples) == 0:\n",
    "        return {\n",
    "            \"run\": name,\n",
    "            \"n_samples\": 0,\n",
    "            \"precision_FAIL\": np.nan,\n",
    "            \"recall_FAIL\": np.nan,\n",
    "            \"f1_FAIL\": np.nan,\n",
    "            \"kappa\": np.nan,\n",
    "        }, df_samples\n",
    "\n",
    "    df_samples = df_samples.copy()\n",
    "    df_samples['pred_label'] = df_samples.apply(llm_evaluate, axis=1)\n",
    "    \n",
    "    # Normalize true labels\n",
    "    df_samples['label'] = (\n",
    "        df_samples['label']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "    )\n",
    "\n",
    "    # Normalize predicted labels\n",
    "    df_samples['pred_label'] = (\n",
    "        df_samples['pred_label']\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "        .str.replace('.', '', regex=False)\n",
    "    )\n",
    "\n",
    "    # Convert to booleans: is this a FAIL?\n",
    "    y_true = df_samples['label']\n",
    "    y_pred = df_samples['pred_label']\n",
    "\n",
    "    y_true_fail = (y_true == 'FAIL')\n",
    "    y_pred_fail = (y_pred == 'FAIL')\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_fail,\n",
    "        y_pred_fail,\n",
    "        average='binary',\n",
    "        pos_label=True,\n",
    "    )\n",
    "\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    row = {\n",
    "        \"run\": name,\n",
    "        \"n_samples\": len(df_samples),\n",
    "        \"precision_FAIL\": precision,\n",
    "        \"recall_FAIL\": recall,\n",
    "        \"f1_FAIL\": f1,\n",
    "        \"kappa\": kappa,\n",
    "    }\n",
    "    return row, df_samples\n",
    "\n",
    "# Example: use on test set after you're happy with dev performance\n",
    "metrics, df_test_scored = run_eval(df_test, name=\"baseline_prompt\")\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
